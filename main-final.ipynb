{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS581: Ridesharing Project\n",
    "## Team 5\n",
    "\n",
    "The following code handles preprocessing rides, filtering them, generating the ridesharing graph, and running the map-matching algorithms.\n",
    "\n",
    "Please find required packages in `requirements.txt`. You can install them via `pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "from h3 import h3\n",
    "import copy\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load rides and distances\n",
    "\n",
    "- Load valid source and destination hexagons\n",
    "- Load rides with relevant columns\n",
    "- Convert pickup/dropoff times to datetime objects\n",
    "- Filter rides by specifying pickup dates and months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(401458, 10)\n",
      "CPU times: user 36.3 s, sys: 3.79 s, total: 40.1 s\n",
      "Wall time: 40.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_path = \"2016-06.csv\" # PATH TO CSV FILE FROM NYC TAXI DATASET\n",
    "s2s = pd.read_csv('data/db/s2s.csv').pickup_h3.unique()\n",
    "d2d = pd.read_csv('data/db/d2d.csv').dropoff_h3.unique()\n",
    "cols_to_keep = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', 'pickup_longitude',\n",
    "       'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'fare_amount', 'total_amount', 'RatecodeID']\n",
    "\n",
    "d = pd.read_csv('data/nyc_taxi/' + file_path, usecols=cols_to_keep)\n",
    "d.columns = ['pickup_datetime', 'dropoff_datetime', 'trip_distance', 'pickup_longitude', \n",
    "            'pickup_latitude', 'rate_code', 'dropoff_longitude', 'dropoff_latitude', 'fare_amount', \n",
    "            'total_amount']\n",
    "d['pickup_datetime'] = pd.to_datetime(d['pickup_datetime'])\n",
    "d['dropoff_datetime'] = pd.to_datetime(d['dropoff_datetime'])\n",
    "date, month = 10, 6\n",
    "d = d[(d['pickup_datetime'].dt.day == date) & (d['pickup_datetime'].dt.month == month)]\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify time if time constraints given\n",
    "- Given a specific day, filter by the start and end times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318665, 10)\n"
     ]
    }
   ],
   "source": [
    "start_date = '2016-06-10 08:00:00' # Start date with time\n",
    "end_date = '2016-06-10 23:59:59' # End date with time\n",
    "mask = (d['pickup_datetime'] >= start_date) & (d['dropoff_datetime'] <= end_date)\n",
    "d_day = d[mask].reset_index().drop('index', axis=1)\n",
    "print(d_day.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert (lat, long) to H3 hexagon identifiers and filter by valid hexagons\n",
    "- Snap pickup and dropoff latitude, longitude pairs to nearest hexagon\n",
    "- Filter them by valid hexagons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3585, 12)\n",
      "CPU times: user 22.5 s, sys: 415 ms, total: 23 s\n",
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "d_day['pickup_h3'] = d_day.apply(lambda x: h3.geo_to_h3(x['pickup_latitude'], x['pickup_longitude'], 8), axis=1)\n",
    "d_day['dropoff_h3'] = d_day.apply(lambda x: h3.geo_to_h3(x['dropoff_latitude'], x['dropoff_longitude'], 10), axis=1)\n",
    "d_day = d_day[d_day['pickup_h3'].isin(s2s)].reset_index()\n",
    "d_day = d_day[d_day['dropoff_h3'].isin(d2d)].reset_index()\n",
    "d_day = d_day.drop(['index', 'level_0'], axis=1)\n",
    "print(d_day.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate pool window times\n",
    "- Ceil pickup times to the nearest pool window time for generating pool groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ceil_dt(dt, delta):\n",
    "    return datetime.min + math.ceil((dt - datetime.min) / delta) * delta\n",
    "\n",
    "pool_time_window = 10 # Change pool time window\n",
    "d_day['pool_window'] = d_day['pickup_datetime'].apply(lambda x: ceil_dt(x.to_pydatetime(), timedelta(minutes=pool_time_window)))\n",
    "d_day['duration'] = (d_day['dropoff_datetime'] - d_day['pickup_datetime']).dt.seconds\n",
    "d_day['delay'] = d_day['duration'].apply(lambda x: x*0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load precomputed distances and durations\n",
    "- Load distances and durations for source-to-source, source-to-destination, and destination-to-destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13764208, 2)\n",
      "CPU times: user 10.1 s, sys: 2.97 s, total: 13.1 s\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s2s_path = 'data_h3/db/s2s.csv'\n",
    "s2d_path = 'data_h3/db/s2d.csv'\n",
    "d2d_path = 'data_h3/db/d2d.csv'\n",
    "\n",
    "s2s = pd.read_csv(s2s_path)\n",
    "s2d = pd.read_csv(s2d_path)\n",
    "d2d = pd.read_csv(d2d_path)\n",
    "org_dist = pd.concat([s2s, s2d, d2d])\n",
    "dist = org_dist.set_index(['pickup_h3', 'dropoff_h3'])\n",
    "del s2s, s2d, d2d\n",
    "print(dist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Definition and Creation\n",
    "- Specify social affinity matrices\n",
    "- Create custom Node classes with required fields\n",
    "- Function for generating all possible ride combinations\n",
    "- Function to calculate edge weights\n",
    "- Sort the graphs by the largest size first (optimization technique for multiprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6214133cbd11485f82baac8ba6257fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=93), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 9.52 s, sys: 444 ms, total: 9.97 s\n",
      "Wall time: 9.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "professions = ['musician', 'engineer', 'doctor', 'lawyer', 'actor']\n",
    "languages = ['english', 'french', 'spanish', 'hindi']\n",
    "aff_prof = [\n",
    "    [1, 0.4, 0.5, 0.1, 0.85], \n",
    "    [0.4, 1, 0.8, 0.4, 0.3], \n",
    "    [0.5, 0.8, 1, 0.6, 0.2], \n",
    "    [0.1, 0.4, 0.6, 1, 0.7], \n",
    "    [0.85, 0.3, 0.2, 0.7, 1]\n",
    "]\n",
    "aff_lang = [\n",
    "    [1, 0.6, 0.78, 0.7],\n",
    "    [0.6, 1, 0.7, 0.2], \n",
    "    [0.78, 0.7, 1, 0.3],\n",
    "    [0.7, 0.2, 0.3, 1]\n",
    "]\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, data, id):\n",
    "        self.id = id\n",
    "        self.pickup_time = data.pickup_datetime\n",
    "        self.dropoff_time = data.dropoff_datetime\n",
    "        self.pickup_loc = (data.pickup_longitude, data.pickup_latitude)\n",
    "        self.dropoff_loc = (data.dropoff_longitude, data.dropoff_latitude)\n",
    "        self.amt = data.fare_amount\n",
    "        self.total_amt = data.total_amount\n",
    "        self.pickup_h3 = data.pickup_h3\n",
    "        self.dropoff_h3 = data.dropoff_h3\n",
    "        self.delay = data.delay\n",
    "        self.trip_dist = dist.loc[(data.pickup_h3, data.dropoff_h3), ['distance']]['distance']\n",
    "        self.duration = dist.loc[(data.pickup_h3, data.dropoff_h3), ['duration']]['duration']\n",
    "        self.profession = np.random.choice(professions)\n",
    "        self.language = np.random.choice(languages)\n",
    "        \n",
    "def generate(a, b):\n",
    "    pa_pb, dur_pa_pb = dist.loc[(a.pickup_h3, b.pickup_h3), ['distance', 'duration']]\n",
    "    pb_pa, dur_pb_pa = dist.loc[(b.pickup_h3, a.pickup_h3), ['distance', 'duration']]\n",
    "    da_db, dur_da_db = dist.loc[(a.dropoff_h3, b.dropoff_h3), ['distance', 'duration']]\n",
    "    db_da, dur_db_da = dist.loc[(b.dropoff_h3, a.dropoff_h3), ['distance', 'duration']]\n",
    "    \n",
    "    pb_da, dur_pb_da = dist.loc[(b.pickup_h3, a.dropoff_h3), ['distance', 'duration']]\n",
    "    pb_db, dur_pb_db = dist.loc[(b.pickup_h3, b.dropoff_h3), ['distance', 'duration']]\n",
    "    pa_db, dur_pa_db = dist.loc[(a.pickup_h3, b.dropoff_h3), ['distance', 'duration']]\n",
    "    pa_da, dur_pa_da = dist.loc[(a.pickup_h3, a.dropoff_h3), ['distance', 'duration']]\n",
    "    \n",
    "    i = pa_pb + pb_da + da_db\n",
    "    j = pa_pb + pb_db + db_da\n",
    "    k = pb_pa + pa_db + db_da\n",
    "    l = pb_pa + pa_da + da_db\n",
    "    \n",
    "    d_i = dur_pa_pb + dur_pb_da + dur_da_db\n",
    "    d_ia, d_ib = dur_pa_pb + dur_pb_da, d_i\n",
    "    \n",
    "    d_j = dur_pa_pb + dur_pb_db + dur_db_da\n",
    "    d_ja, d_jb = d_j, dur_pa_pb + dur_pb_db\n",
    "    \n",
    "    d_k = dur_pb_pa + dur_pa_db + dur_db_da\n",
    "    d_ka, d_kb = d_k, dur_pb_pa + dur_pa_db\n",
    "    \n",
    "    d_l = dur_pb_pa + dur_pa_da + dur_da_db\n",
    "    d_la, d_lb = dur_pb_pa + dur_pa_da, d_l\n",
    "    return [(i, d_i, d_ia, d_ib), (j, d_j, d_ja, d_jb), (k, d_k, d_ka, d_kb), (l, d_l, d_la, d_lb)]\n",
    "    \n",
    "graphs = []\n",
    "for time, df in tqdm_notebook(d_day.groupby(['pool_window']), \n",
    "                              total=len(d_day.groupby(['pool_window']))):\n",
    "    nodes = []\n",
    "    df = df.reset_index()\n",
    "    for idx, row in df.iterrows():\n",
    "        nodes.append(Node(df.iloc[idx], idx))\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    graphs.append(G)\n",
    "    \n",
    "def target_func(gr):\n",
    "    def edge_weight_calc(a, b, w1=0.25, w2=0.75, w3=0.80):\n",
    "            t_min = float('inf')\n",
    "            d_min = float('inf')\n",
    "            R = generate(a, b)\n",
    "            t_a = a.trip_dist\n",
    "            t_b = b.trip_dist\n",
    "            d_a = a.duration\n",
    "            d_b = b.duration\n",
    "            for (t, d, dp_a, dp_b) in R:\n",
    "                if t <= (t_a + t_b) and t <= t_min and dp_a <= (d_a + a.delay) and dp_b <= (d_b + b.delay) and d <= d_min:\n",
    "                    t_min = t\n",
    "                    d_min = d\n",
    "            pf = aff_prof[professions.index(a.profession)][professions.index(b.profession)]\n",
    "            la = aff_lang[languages.index(a.language)][languages.index(b.language)]\n",
    "            \n",
    "            return [w1*(t_a + t_b - t_min), w2*(d_a + d_b - d_min), w3*(pf+la)/2]\n",
    "    for a in gr:\n",
    "        for b in gr:\n",
    "            if a.id == b.id: continue\n",
    "            [t_w, d_w, soc] = edge_weight_calc(a, b)\n",
    "            if t_w != float('-inf') and d_w != float('-inf'):\n",
    "                gr.add_edge(a, b, weight={'distance': t_w, 'duration': d_w, 'final': soc*(t_w+d_w)})\n",
    "    return gr\n",
    "\n",
    "graphs = sorted(graphs, key=lambda x: x.number_of_nodes(), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge weight calculation\n",
    "- Run edge weight calculation in parallel scaled by the number of available physical CPUs\n",
    "(This may take a while depending on the number of available CPUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.1 s, sys: 2.23 s, total: 4.33 s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p = Pool(processes=cpu_count())\n",
    "data = p.map(target_func, graphs)\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Weighted Matching Algorithm\n",
    "- Run `max_weight_matching` algorithm on all our pool graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c39185016b4a43ae259b1e1a969f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=93), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "weight_matches = []\n",
    "for graph in tqdm_notebook(data, total=len(graphs)):\n",
    "    match = nx.max_weight_matching(graph, weight='final', maxcardinality=True)\n",
    "    g_match = nx.Graph()\n",
    "    for ii in match:\n",
    "        g_match.add_edge(ii[0],ii[1])\n",
    "    weight_matches.append(g_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilization\n",
    "- Calculate the utilization for each pool graph and average them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.17672320116023\n",
      "97.28280247939068\n"
     ]
    }
   ],
   "source": [
    "org_nodes = [x.number_of_nodes() for x in graphs]\n",
    "weight_match_nodes = [x.number_of_nodes() for x in weight_matches]\n",
    "\n",
    "print(sum([(y/x) for x,y in zip(org_nodes, weight_match_nodes)]) / len(graphs)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average distance saved per pool as a % of total distance of individual rides\n",
    "- Calculate the average distance saved per pool as a percentage of total distance of individual rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b407dcaba084e49bb0c7531bafddf43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=93), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "55.738847205366106\n"
     ]
    }
   ],
   "source": [
    "new_dist, org_dist = [], []\n",
    "\n",
    "for idx in tqdm_notebook(range(len(weight_matches)), total=len(weight_matches)):\n",
    "    org_d, d = 0, 0\n",
    "    individual_nodes = set()\n",
    "    for node in data[idx].nodes:\n",
    "        org_d += node.trip_dist\n",
    "        individual_nodes.add(node)\n",
    "    for edge in weight_matches[idx].edges:\n",
    "        individual_nodes.remove(edge[0])\n",
    "        individual_nodes.remove(edge[1])\n",
    "        d += data[idx].get_edge_data(edge[0], edge[1])['weight']['distance']*4\n",
    "    for node in individual_nodes:\n",
    "        d += node.trip_dist\n",
    "    new_dist.append(d)\n",
    "    org_dist.append(org_d)\n",
    "\n",
    "print(sum([(1-x/y) for x, y in zip(new_dist, org_dist)])/len(org_dist) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average number of trips saved per pool as a % of number of individual trips\n",
    "- Calculate the average number of trips saved per pool as a percentage of number of individual rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.64140123969537\n"
     ]
    }
   ],
   "source": [
    "saved_rides = []\n",
    "for idx in range(len(data)):\n",
    "    num_ind_trips = len(data[idx].nodes)\n",
    "    num_pooled_trips = len(weight_matches[idx].edges)\n",
    "    saved_rides.append(num_pooled_trips/num_ind_trips * 100)\n",
    "print(sum(saved_rides)/len(saved_rides))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
